---
title: "boosting"
output: html_document
date: "2023-05-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r setup, message=FALSE}
# load libraries
library(tidyverse)
library(caret)
library(vip)
library(rpart)
library(rpart.plot)
library(visdat)
library(gtsummary)
library(pROC)
library(e1071)
library(kernlab)
```

```{r, results='hide'}
# import and clean dataset 
load("data/recovery.rdata")

dat %>% 
  na.omit() 

set.seed(5929) 
final1 =
  dat[sample(1:10000, 2000),] %>% 
  janitor::clean_names() %>%
  mutate (gender=as.factor(gender),
          hypertension=as.factor(hypertension),
          diabetes=as.factor(diabetes),
          vaccine=as.factor(vaccine),
          severity=as.factor(severity),
          study=as.factor(study))

set.seed(3186) 
final2 =
  dat[sample(1:10000, 2000),] %>% 
  janitor::clean_names() %>%
  mutate (
          hypertension=as.factor(hypertension),
          diabetes=as.factor(diabetes),
          vaccine=as.factor(vaccine),
          severity=as.factor(severity),
          study=as.factor(study))

# merge 2 dataset 
final = merge(final1, final2, all=TRUE) %>% 
  select(-id)

# add a new binary variable
final = merge(final1, final2, all=TRUE) %>% 
  select(-id) %>%
  mutate(status=case_when(recovery_time <=30 ~ 0,
                          recovery_time >30 ~ 1,
                          ) ) 

```
```{r}
dat_number = c(1, 5, 6, 7, 10, 11)
theme1 <- trellis.par.get()
theme1$plot.symbol$col <- rgb(.2, .4, .2, .5)
theme1$plot.symbol$pch <- 16
theme1$plot.line$col <- rgb(.8, .1, .1, 1)
theme1$plot.line$lwd <- 2
theme1$strip.background$col <- rgb(.0, .2, .6, .2)
trellis.par.set(theme1)
fp=featurePlot(x = final[, dat_number],
            y = final$recovery_time, 
            plot = "scatter", 
            type = c("p", "smooth"),
            layout = c(3, 2))
fp
```
```{r load_data}
final$gender<- as.factor(ifelse(final$gender == 1, "male", "female"))
final$status<- as.factor(ifelse(final$status == 1, "rt.mt.30", "rt.lt.30"))
```

```{r}
table=final %>%
  tbl_summary(by = status, missing_text = "Missing/NA") %>%
  add_p(pvalue_fun = ~style_pvalue(.x, digits = 2)) %>%
  add_overall() %>%
  add_n() %>%
  modify_header(label ~ "**Variable**") %>%
  modify_spanning_header(c("stat_1", "stat_2") ~ "**Subject Type**") %>%
  modify_footnote(
    all_stat_cols() ~ "Median (IQR) or Frequency (%)"
  )
table
```


```{r train_test_split_bin}
set.seed(2023)
rowTrain <- createDataPartition(y = final$status, p = 0.8, list = FALSE)

final.train.bin <- final[rowTrain,]%>%select(-recovery_time) # delete recovery time

# make the binary column to the first column
final.train.bin = final.train.bin[, c(ncol(final.train.bin), 1:(ncol(final.train.bin)-1))] 

final.test.bin <- final[-rowTrain,]%>%select(-recovery_time)
final.test.bin = final.test.bin[, c(ncol(final.test.bin), 1:(ncol(final.test.bin)-1))]



final.train.x.bin <- model.matrix(status ~ ., data = final.train.bin)[,-1]
final.train.y.bin <- final[rowTrain,]$status

final.test.x.bin <- model.matrix(status ~ ., data = final.test.bin)[,-1]
final.test.y.bin <- final[-rowTrain,]$status

```

```{r train_test_split_con}

set.seed(2023)
rowTrain1 <- createDataPartition(y = final$recovery_time, p = 0.8, list = FALSE)
final.train.con <- final[rowTrain1,]%>%select(-status) # delete binary

# make the continuous column to the first column
final.train.con = final.train.con[, c(ncol(final.train.con), 1:(ncol(final.train.con)-1))] 

final.test.con <- final[-rowTrain1,]%>%select(-status) 
final.test.con = final.test.con[, c(ncol(final.test.con), 1:(ncol(final.test.con)-1))]



final.train.x.con <- model.matrix(recovery_time ~ ., data = final.train.con)[,-1]
final.train.y.con <- final[rowTrain1,]$recovery_time

final.test.x.con <- model.matrix(recovery_time ~ ., data = final.test.con)[,-1]
final.test.y.con <- final[-rowTrain1,]$recovery_time

```


```{r ctrl}
set.seed(2023)
ctrl <- trainControl(method = "cv",
                     classProbs = TRUE, 
                     summaryFunction = twoClassSummary)
```

```{r adaboost}
library(parallel)
# Calculate the number of cores
num_cores <- detectCores() - 1
library(doParallel)
# create the cluster for caret to use
# CPU usage may go up to 100%
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)
gbm.grid <- expand.grid(n.trees = c(2000,3000,4000,5000),#
                        interaction.depth = 1:6,
                        shrinkage = c(0.0005,0.001,0.002),#
                        n.minobsinnode = 1)
set.seed(2023)
gbm.fit <- train(status ~ . ,
                  final.train.bin,
                  tuneGrid = gbm.grid,
                  trControl = ctrl,
                  method = "gbm",
                  distribution = "adaboost",
                  metric = "ROC",
                  verbose = FALSE)


gbm=ggplot(gbm.fit, highlight = TRUE)
ggsave(file="plots/gbm.png",gbm,width=8,height=5)

gbm.fit$bestTune
gbm.pred <- predict(gbm.fit, newdata = final.test.bin, type = "raw")
gbm.pred
error.rate.gbm <- mean(gbm.pred != final.test.y.bin)
error.rate.gbm

```

```{r elasticnet}
ctrl_1 = trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(2023)
# build elastic net model with caret
elnet_model = train(final.train.x.con, final.train.y.con, 
                    method = "glmnet",
                    tuneGrid = expand.grid(alpha = seq(0, 1, length=21),
                                           lambda = exp(seq(7, -1, length=50))),
                    trControl = ctrl_1)
myCol<- rainbow(21)
myPar <- list(superpose.symbol = list(col = myCol),
superpose.line = list(col = myCol))
plot(elnet_model, par.settings = myPar)
# show the best lambda and alpha combination with lowest cv rmse
elnet_model$bestTune
```


```{r}
# Multivariate Adaptive Regression Splines (MARS) Model
set.seed(2023)

# perform a grid search to identify optimal combination of hyperparameters
mars_grid <- expand.grid(degree = 1:3, nprune = 2:25)

# fit the model 
mars.fit <- train(x = final.train.x.con, y = final.train.y.con, 
                  method = "earth",
                  tuneGrid = mars_grid,
                  trControl = ctrl_1)

# check plot
ggplot(mars.fit) + labs(title = "Multivariate Adaptive Regression Splines (MARS) Model")

# variable importance plot
vip(mars.fit$finalModel)

# check results
mars.fit$bestTune
coef(mars.fit$finalModel)

```
```{r}
# Classification Tree 
set.seed(2023)

rpart.fit <- train(status ~ . ,
                  final.train.bin,
                   method = "rpart",
                   tuneGrid = data.frame(cp = exp(seq(-10,-4, len = 50))),
                   trControl = ctrl,
                   metric = "ROC")

# check plot
ggplot(rpart.fit, highlight = TRUE) + labs(title = "ROC of Classification Tree")
rpart.plot(rpart.fit$finalModel)

# check results
rpart.fit$bestTune
```

### SVM radial kernel

```{r}
library(parallel)
# Calculate the number of cores
num_cores <- detectCores() 
library(doParallel)
# create the cluster for caret to use
# CPU usage may go up to 100%
cl <- makePSOCKcluster(num_cores)
registerDoParallel(cl)


set.seed(2023)
radial.tune <- tune.svm(status ~ . ,
                        data = final.train.bin,
                        kernel = "radial",
                        cost = exp(seq(-1,5,len=20)),
                        gamma = exp(seq(-4,3,len=20)))
plot(radial.tune, transform.y = log, transform.x = log,
color.palette = terrain.colors)

best.radial = radial.tune$best.model

# test error rate

pred.radial = predict(best.radial, newdata=final.test.bin)
confusionMatrix(data=pred.radial,reference = final.test.y.bin)

stopCluster(cl)
registerDoSEQ()
```




## GAM
```{r}
set.seed(2023)
gam.fit <- train(x = final.train.x.con, y = final.train.y.con,
                 method = "gam", tuneGrid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE)), 
                 trControl = ctrl_1)
gam.fit$bestTune
gam.fit$finalModel
plot(gam.fit$finalModel, shade = TRUE, pages = 4)
summary(gam.fit$finalModel)
```



